{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/ScottTeran/ga_capstone/blob/main/code/extraction_vision_api_clean.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### intro\n",
    "This notebook was originally created in Google Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lgZVdeBomn8u",
    "outputId": "23d9db85-a3c5-4748-fdc5-d04bd5bdc609"
   },
   "outputs": [],
   "source": [
    "# this has to be installed for each new runtime in Google Colab\n",
    "# pip install google-cloud-vision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5jfV-ybumyF8",
    "outputId": "0631d874-5a56-4250-a8cd-118b78f8ea05"
   },
   "outputs": [],
   "source": [
    "# this has to be installed for each new runtime in Google Colab\n",
    " # pip install google-cloud-storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "uMID33KGCENG"
   },
   "outputs": [],
   "source": [
    "# using this to check versions\n",
    "\n",
    "# pip freeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "5yfO0SPsqJcT"
   },
   "outputs": [],
   "source": [
    "# this helped with accessing GCS https://stackoverflow.com/questions/45501082/set-google-application-credentials-in-python-project-to-use-google-api\n",
    "\n",
    "import os\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"]=\"/content/ra-lafferty-13f7704670eb.json\" # this is my API key file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "D2_4eK4RlPzB"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "from google.cloud import vision\n",
    "from google.cloud import storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "S3QwKDyUmd1a"
   },
   "outputs": [],
   "source": [
    "# this code is from Silvia Zeamer | https://towardsdatascience.com/how-to-extract-the-text-from-pdfs-using-python-and-the-google-cloud-vision-api-7a0a798adc13\n",
    "# the Vision API documentation | https://cloud.google.com/vision/docs/fulltext-annotations\n",
    "\n",
    "def async_detect_document(gcs_source_uri, gcs_destination_uri):\n",
    "    \n",
    "    mime_type = 'application/pdf'\n",
    "    \n",
    "    # how many PDF pages will go in each file (100 is max)\n",
    "    batch_size =  100 \n",
    "    \n",
    "    # the tool that annotates text in a PDF\n",
    "    client = vision.ImageAnnotatorClient()\n",
    "    \n",
    "    feature = vision.Feature(\n",
    "        type_ = vision.Feature.Type.DOCUMENT_TEXT_DETECTION)\n",
    "    \n",
    "    # telling the Vision API that source type is PDF (mime_type) and where it can be found\n",
    "    gcs_source = vision.GcsSource(uri = gcs_source_uri)\n",
    "    input_config = vision.InputConfig(\n",
    "        gcs_source  = gcs_source, mime_type = mime_type)\n",
    "    \n",
    "    # generate JSON files with 100 pages worth of data each\n",
    "    gcs_destination = vision.GcsDestination(uri = gcs_destination_uri)\n",
    "    output_config = vision.OutputConfig(\n",
    "        gcs_destination = gcs_destination, \n",
    "        batch_size = batch_size)\n",
    "    \n",
    "    # an asynchronous request using input and output configs\n",
    "    async_request = vision.AsyncAnnotateFileRequest(\n",
    "        features = [feature], input_config = input_config,\n",
    "        output_config = output_config)\n",
    "    \n",
    "    # batch annotate files using client and asyn_request set up earlier\n",
    "    operation = client.async_batch_annotate_files(\n",
    "        requests = [async_request])\n",
    "    \n",
    "    print('Waiting for the operation to finish.')\n",
    "    operation.result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w0ZeEEZA3xnU",
    "outputId": "119ca380-51d1-4fb9-cefd-5fd90190c3ae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for the operation to finish.\n"
     ]
    }
   ],
   "source": [
    "async_detect_document('gs://ra_lafferty_pdfs/the_devil_is_dead.pdf', 'gs://ra_lafferty_pdfs/txt_files/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "emDFCPPcLN8D"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "F9mAQydZxkD6"
   },
   "outputs": [],
   "source": [
    "# so that I don't have to copy-paste so much...\n",
    "base = 'gs://ra_lafferty_pdfs/txt_files/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "_OOCHarNvRXw"
   },
   "outputs": [],
   "source": [
    "def make_blob_list(gcs_destination_uri, verbose=False):\n",
    "\n",
    "  '''\n",
    "  Returns a blob list based off of a GCS URI. Blobs are basically\n",
    "  GC objects.\n",
    "  '''\n",
    "  \n",
    "  # Client to bundle configuration needed for API requests\n",
    "  storage_client = storage.Client()\n",
    "\n",
    "  # Generate vars for bucket request\n",
    "  match = re.match(r'gs://([^/]+)/(.+)', gcs_destination_uri)\n",
    "  bucket_name = match.group(1)\n",
    "  prefix = match.group(2)\n",
    "\n",
    "  # Generate bucket var\n",
    "  bucket = storage_client.get_bucket(bucket_name)\n",
    "\n",
    "  # Make blob list\n",
    "  blob_list = list(bucket.list_blobs(prefix=prefix))\n",
    "\n",
    "  # For troubleshooting purposes, print blob names\n",
    "  if verbose:\n",
    "    for blob in blob_list:\n",
    "      print(blob.name)\n",
    "\n",
    "  return blob_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "7ziGbZQyyrMt"
   },
   "outputs": [],
   "source": [
    "blob_list = make_blob_list(base, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "nhncL5HN2Xol"
   },
   "outputs": [],
   "source": [
    "def blob_to_text(blob, verbose=False):\n",
    "  '''\n",
    "  Accepts one blob and returns one text, for all\n",
    "  pages processed and contained in the blob. Optional\n",
    "  verbose parameter for troubleshooting.\n",
    "  '''\n",
    "  \n",
    "  if verbose:\n",
    "    print(f'Now processing: {blob.name}')\n",
    "\n",
    "  blob_string = blob.download_as_string()\n",
    "  blob_json = json.loads(blob_string)\n",
    "  responses = [r for r in blob_json['responses'] if 'fullTextAnnotation' in r.keys()]\n",
    "  texts = [each['fullTextAnnotation']['text'] for each in responses]\n",
    "  \n",
    "  if verbose:\n",
    "    print('Response count:', len(blob_json['responses']))\n",
    "    print('Texts count:', len(texts))\n",
    "  \n",
    "  return ''.join(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "FvuZr2gM4jI9"
   },
   "outputs": [],
   "source": [
    "def write_to_text(gcs_destination_uri, verbose=False, write=True):\n",
    "\n",
    "  '''\n",
    "  Accepts a GCS URI and returns a text file containing all texts for\n",
    "  blobs in the GCS destination. Optional verbose parameter for\n",
    "  troubleshooting. Default write to disk; this can be overwritten\n",
    "  by setting write=False.\n",
    "  '''\n",
    "  \n",
    "  blob_list = make_blob_list(gcs_destination_uri, verbose=verbose)\n",
    "  blob_texts = [blob_to_text(blob, verbose=verbose) for blob in blob_list]\n",
    "  texts = ''.join(blob_texts)\n",
    "\n",
    "  if write:\n",
    "    with open(\"lafferty.txt\", \"w\") as f:\n",
    "      f.write(texts)\n",
    "\n",
    "  return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WxyJ-OGb5LDw",
    "outputId": "44c48c9d-c324-4304-d11a-e9b0bc1549ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "txt_files/output-1-to-100.json\n",
      "txt_files/output-101-to-200.json\n",
      "txt_files/output-201-to-226.json\n",
      "txt_files/output-201-to-285.json\n",
      "Now processing: txt_files/output-1-to-100.json\n",
      "Response count: 100\n",
      "Texts count: 97\n",
      "Now processing: txt_files/output-101-to-200.json\n",
      "Response count: 100\n",
      "Texts count: 100\n",
      "Now processing: txt_files/output-201-to-226.json\n",
      "Response count: 26\n",
      "Texts count: 26\n",
      "Now processing: txt_files/output-201-to-285.json\n",
      "Response count: 85\n",
      "Texts count: 85\n"
     ]
    }
   ],
   "source": [
    "t = write_to_text(base, verbose=True, write=True)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "extraction_vision_api_clean",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
