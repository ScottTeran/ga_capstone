{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "01_extraction_vision_api_clean",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ScottTeran/ga_capstone/blob/main/code/01_extraction_vision_api_clean.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXXuHRTYX8YE"
      },
      "source": [
        "# Text extraction using Google Cloud Vision API\n",
        "Credit goes to Silvia Zeamer for the first 60% of this notebook. The other 40% goes to Caroline Schmidt for figuring out how to get JSON files back out of GCS and into txt format. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lgZVdeBomn8u"
      },
      "source": [
        "# this has to be installed for each new runtime on Google Colab\n",
        "\n",
        "# !pip install google-cloud-vision"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5jfV-ybumyF8"
      },
      "source": [
        "# this has to be installed for each new runtime\n",
        "\n",
        "# !pip install google-cloud-storage"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uMID33KGCENG"
      },
      "source": [
        "# using this to check versions\n",
        "\n",
        "# pip freeze"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5yfO0SPsqJcT"
      },
      "source": [
        "# this helped with accessing GCS [https://stackoverflow.com/questions/45501082/set-google-application-credentials-in-python-project-to-use-google-api]\n",
        "\n",
        "import os\n",
        "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"]=\"/content/ra-lafferty-13f7704670eb.json\" # this is my API key file"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D2_4eK4RlPzB"
      },
      "source": [
        "import json\n",
        "import re\n",
        "\n",
        "from google.cloud import vision\n",
        "from google.cloud import storage"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S3QwKDyUmd1a"
      },
      "source": [
        "# this code is from Silvia Zeamer [https://towardsdatascience.com/how-to-extract-the-text-from-pdfs-using-python-and-the-google-cloud-vision-api-7a0a798adc13]\n",
        "# also, code included from Vision API documentation [https://cloud.google.com/vision/docs/fulltext-annotations]\n",
        "\n",
        "def async_detect_document(gcs_source_uri, gcs_destination_uri):\n",
        "    \n",
        "    mime_type = 'application/pdf'\n",
        "    \n",
        "    # how many PDF pages will go in each file (100 is max)\n",
        "    batch_size =  100 \n",
        "    \n",
        "    # the tool that annotates text in a PDF\n",
        "    client = vision.ImageAnnotatorClient()\n",
        "    \n",
        "    feature = vision.Feature(\n",
        "        type_ = vision.Feature.Type.DOCUMENT_TEXT_DETECTION)\n",
        "    \n",
        "    # telling the Vision API that source type is PDF (mime_type) and where it can be found\n",
        "    gcs_source = vision.GcsSource(uri = gcs_source_uri)\n",
        "    input_config = vision.InputConfig(\n",
        "        gcs_source  = gcs_source, mime_type = mime_type)\n",
        "    \n",
        "    # generate JSON files with 100 pages worth of data each\n",
        "    gcs_destination = vision.GcsDestination(uri = gcs_destination_uri)\n",
        "    output_config = vision.OutputConfig(\n",
        "        gcs_destination = gcs_destination, \n",
        "        batch_size = batch_size)\n",
        "    \n",
        "    # an asynchronous request using input and output configs\n",
        "    async_request = vision.AsyncAnnotateFileRequest(\n",
        "        features = [feature], input_config = input_config,\n",
        "        output_config = output_config)\n",
        "    \n",
        "    # batch annotate files using client and asyn_request set up earlier\n",
        "    operation = client.async_batch_annotate_files(\n",
        "        requests = [async_request])\n",
        "    \n",
        "    print('Waiting for the operation to finish.')\n",
        "    operation.result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w0ZeEEZA3xnU",
        "outputId": "119ca380-51d1-4fb9-cefd-5fd90190c3ae"
      },
      "source": [
        "async_detect_document('gs://ra_lafferty_pdfs/the_devil_is_dead.pdf', 'gs://ra_lafferty_pdfs/txt_files/')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Waiting for the operation to finish.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "emDFCPPcLN8D"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F9mAQydZxkD6"
      },
      "source": [
        "# so that I don't have to copy-paste so much... (note from Caroline Schmidt)\n",
        "base = 'gs://ra_lafferty_pdfs/txt_files/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_OOCHarNvRXw"
      },
      "source": [
        "def make_blob_list(gcs_destination_uri, verbose=False):\n",
        "\n",
        "  '''\n",
        "  Returns a blob list based off of a GCS URI. Blobs are basically\n",
        "  GC objects.\n",
        "  '''\n",
        "  \n",
        "  # client to bundle configuration needed for API requests\n",
        "  storage_client = storage.Client()\n",
        "\n",
        "  # generate vars for bucket request\n",
        "  match = re.match(r'gs://([^/]+)/(.+)', gcs_destination_uri)\n",
        "  bucket_name = match.group(1)\n",
        "  prefix = match.group(2)\n",
        "\n",
        "  # generate bucket var\n",
        "  bucket = storage_client.get_bucket(bucket_name)\n",
        "\n",
        "  # make blob list\n",
        "  blob_list = list(bucket.list_blobs(prefix=prefix))\n",
        "\n",
        "  # For troubleshooting purposes, print blob names\n",
        "  if verbose:\n",
        "    for blob in blob_list:\n",
        "      print(blob.name)\n",
        "\n",
        "  return blob_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ziGbZQyyrMt"
      },
      "source": [
        "blob_list = make_blob_list(base, verbose=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nhncL5HN2Xol"
      },
      "source": [
        "def blob_to_text(blob, verbose=False):\n",
        "  '''\n",
        "  Accepts one blob and returns one text, for all\n",
        "  pages processed and contained in the blob. Optional\n",
        "  verbose parameter for troubleshooting.\n",
        "  '''\n",
        "  \n",
        "  if verbose:\n",
        "    print(f'Now processing: {blob.name}')\n",
        "\n",
        "  blob_string = blob.download_as_string()\n",
        "  blob_json = json.loads(blob_string)\n",
        "  responses = [r for r in blob_json['responses'] if 'fullTextAnnotation' in r.keys()]\n",
        "  texts = [each['fullTextAnnotation']['text'] for each in responses]\n",
        "  \n",
        "  if verbose:\n",
        "    print('Response count:', len(blob_json['responses']))\n",
        "    print('Texts count:', len(texts))\n",
        "  \n",
        "  return ''.join(texts)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FvuZr2gM4jI9"
      },
      "source": [
        "def write_to_text(gcs_destination_uri, verbose=False, write=True):\n",
        "\n",
        "  '''\n",
        "  Accepts a GCS URI and returns a text file containing all texts for\n",
        "  blobs in the GCS destination. Optional verbose parameter for\n",
        "  troubleshooting. Default write to disk; this can be overwritten\n",
        "  by setting write=False.\n",
        "  '''\n",
        "  \n",
        "  blob_list = make_blob_list(gcs_destination_uri, verbose=verbose)\n",
        "  blob_texts = [blob_to_text(blob, verbose=verbose) for blob in blob_list]\n",
        "  texts = ''.join(blob_texts)\n",
        "\n",
        "  if write:\n",
        "    with open(\"lafferty.txt\", \"w\") as f:\n",
        "      f.write(texts)\n",
        "\n",
        "  return texts"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WxyJ-OGb5LDw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44c48c9d-c324-4304-d11a-e9b0bc1549ca"
      },
      "source": [
        "# if running in Google Colab file will export to 'content/'\n",
        "t = write_to_text(base, verbose=True, write=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "txt_files/output-1-to-100.json\n",
            "txt_files/output-101-to-200.json\n",
            "txt_files/output-201-to-226.json\n",
            "txt_files/output-201-to-285.json\n",
            "Now processing: txt_files/output-1-to-100.json\n",
            "Response count: 100\n",
            "Texts count: 97\n",
            "Now processing: txt_files/output-101-to-200.json\n",
            "Response count: 100\n",
            "Texts count: 100\n",
            "Now processing: txt_files/output-201-to-226.json\n",
            "Response count: 26\n",
            "Texts count: 26\n",
            "Now processing: txt_files/output-201-to-285.json\n",
            "Response count: 85\n",
            "Texts count: 85\n"
          ]
        }
      ]
    }
  ]
}